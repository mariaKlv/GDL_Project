{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GDL_Clique_Pooling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3IEVy9yGtRT3",
        "12wx0ly40c0J",
        "0kbfJ3h1tblO"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy1LXmANsxGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31296d5a-7a64-44c8-cbf3-137fbdef6e55"
      },
      "source": [
        "import torch\n",
        "\n",
        "!pip install torch-scatter\n",
        "!pip install torch-sparse\n",
        "!pip install torch-cluster\n",
        "!pip install torch-spline-conv\n",
        "!pip install torch-geometric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.6)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.5.1)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.51.2)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.21.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric) (56.1.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->torch-geometric) (0.34.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIfcNFjRAd3q"
      },
      "source": [
        "from CliquePoolingLayer import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IEVy9yGtRT3"
      },
      "source": [
        "### Implementation Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMaM4y_XUhdA"
      },
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_sparse import SparseTensor\n",
        "from torch import Tensor\n",
        "\n",
        "def test_clique_pool_dual_edge():\n",
        "\n",
        "  # A  B  C  D  E  F  G  H  I  J\n",
        "  # 0. 1. 2. 3. 4. 5. 6. 7. 8. 9\n",
        "\n",
        "  # A.neighbors = [B]             1\n",
        "  # B.neighbors = [A, C, D].      0, 2, 3\n",
        "  # C.neighbors = [B, D, G].      1, 3, 6\n",
        "  # D.neighbors = [B, C, E, F].   1, 2, 4, 5\n",
        "  # E.neighbors = [D, F, J].      3, 5, 9\n",
        "  # F.neighbors = [D, E].         3, 4\n",
        "  # G.neighbors = [C, H, I, J].   2, 7, 8, 9\n",
        "  # H.neighbors = [G, I, J].      6, 8, 9\n",
        "  # I.neighbors = [G, H, J].      6, 7, 9\n",
        "  # J.neighbors = [E, I, G, H].   4, 8, 6, 7\n",
        "\n",
        "\n",
        "  edge_index = torch.transpose(torch.tensor([\n",
        "                            [0, 1],\n",
        "                            [1, 0],\n",
        "                            [1, 2],\n",
        "                            [1, 3],\n",
        "                            [2, 1],\n",
        "                            [2, 3],\n",
        "                            [2, 6],\n",
        "                            [3, 1],\n",
        "                            [3, 2],\n",
        "                            [3, 4],\n",
        "                            [3, 5],\n",
        "                            [4, 3],\n",
        "                            [4, 5],\n",
        "                            [4, 9],\n",
        "                            [5, 3],\n",
        "                            [5, 4],\n",
        "                            [6, 2],\n",
        "                            [6, 7],\n",
        "                            [6, 8],\n",
        "                            [6, 9],\n",
        "                            [7, 6],\n",
        "                            [7, 8],\n",
        "                            [7, 9],\n",
        "                            [8, 6],\n",
        "                            [8, 7],\n",
        "                            [8, 9],\n",
        "                            [9, 4],\n",
        "                            [9, 8],\n",
        "                            [9, 6],\n",
        "                            [9, 7]], dtype=torch.long), 0, 1)\n",
        "  x = torch.rand(10, 40)\n",
        "\n",
        "  data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
        "  cliquePooling = CliquePooling('avg')\n",
        "  dual_x, dual_edges_t, _, dual_batch, _= cliquePooling.forward(x, edge_index)\n",
        "\n",
        "  assert dual_edges_t.tolist() == [[1, 0, 1, 2, 0], [2, 1, 3, 3, 2]], \"Hey, something went wrong!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIy3tI9qVlu9"
      },
      "source": [
        "test_clique_pool_dual_edge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12wx0ly40c0J"
      },
      "source": [
        "### Benchmark testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kbfJ3h1tblO"
      },
      "source": [
        "#### Proteins dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhRTwqQ1mVYz"
      },
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "\n",
        "#https://github.com/rusty1s/pytorch_geometric/blob/cff934ec28fb70d05d434c74111047078667ace8/examples/proteins_topk_pool.py#L25\n",
        "dataset = TUDataset(root='/tmp/PROTEINS', name='PROTEINS')\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "n = len(dataset) // 10\n",
        "b_size = 2\n",
        "test_dataset = dataset[:n]\n",
        "train_dataset = dataset[n:]\n",
        "test_loader = DataLoader(test_dataset, batch_size=b_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=b_size)\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
        "    self.conv2 = GCNConv(16, 32)\n",
        "    self.pool  = CliquePooling('avg')\n",
        "    self.conv3 = GCNConv(32, dataset.num_classes, add_self_loops=False)\n",
        "\n",
        "  def forward(self, data):\n",
        "    ## 1. Obtain node embeddings:\n",
        "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "    # First GCN\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    # Second GCN\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x, d_edge_index, _, d_batch, _ = self.pool(x, edge_index, None, batch)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    # Third GCN\n",
        "    x = self.conv3(x, d_edge_index)\n",
        "\n",
        "    ## 2. Readout layer:\n",
        "    x = global_mean_pool(x, d_batch)\n",
        "\n",
        "    x = F.log_softmax(x, dim=-1)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBnw5b6aYIo0",
        "outputId": "271e21c8-2b49-4676-9456-d4049351d47a"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001) # lr=0.01, weight_decay=5e-4\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "\n",
        "  loss_all = 0\n",
        "  for data in train_loader:\n",
        "    data = data.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, data.y)\n",
        "    loss.backward()\n",
        "    loss_all += data.num_graphs * loss.item()\n",
        "    optimizer.step()\n",
        "  return loss_all / len(train_dataset)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  for data in loader:\n",
        "    data = data.to(device)\n",
        "    pred = model(data).max(dim=1)[1]\n",
        "    correct += pred.eq(data.y).sum().item()\n",
        "  return correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "for epoch in range(1, 500):\n",
        "  loss = train(epoch)\n",
        "  train_acc = test(train_loader)\n",
        "  test_acc = test(test_loader)\n",
        "  print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
        "        format(epoch, loss, train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 0.68174, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 002, Loss: 0.66391, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 003, Loss: 0.65438, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 004, Loss: 0.64462, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 005, Loss: 0.63916, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 006, Loss: 0.63755, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 007, Loss: 0.63598, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 008, Loss: 0.63155, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 009, Loss: 0.63062, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 010, Loss: 0.63143, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 011, Loss: 0.63137, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 012, Loss: 0.62981, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 013, Loss: 0.62712, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 014, Loss: 0.62867, Train Acc: 0.60180, Test Acc: 0.54054\n",
            "Epoch: 015, Loss: 0.62739, Train Acc: 0.64471, Test Acc: 0.61261\n",
            "Epoch: 016, Loss: 0.62395, Train Acc: 0.64471, Test Acc: 0.61261\n",
            "Epoch: 017, Loss: 0.62705, Train Acc: 0.64471, Test Acc: 0.61261\n",
            "Epoch: 018, Loss: 0.62147, Train Acc: 0.64471, Test Acc: 0.61261\n",
            "Epoch: 019, Loss: 0.62417, Train Acc: 0.64371, Test Acc: 0.62162\n",
            "Epoch: 020, Loss: 0.62596, Train Acc: 0.64571, Test Acc: 0.62162\n",
            "Epoch: 021, Loss: 0.62391, Train Acc: 0.64571, Test Acc: 0.61261\n",
            "Epoch: 022, Loss: 0.62123, Train Acc: 0.64571, Test Acc: 0.61261\n",
            "Epoch: 023, Loss: 0.62549, Train Acc: 0.64870, Test Acc: 0.63063\n",
            "Epoch: 024, Loss: 0.62347, Train Acc: 0.64970, Test Acc: 0.63063\n",
            "Epoch: 025, Loss: 0.62298, Train Acc: 0.64970, Test Acc: 0.63964\n",
            "Epoch: 026, Loss: 0.62185, Train Acc: 0.65170, Test Acc: 0.63964\n",
            "Epoch: 027, Loss: 0.62335, Train Acc: 0.65469, Test Acc: 0.63964\n",
            "Epoch: 028, Loss: 0.62060, Train Acc: 0.65569, Test Acc: 0.64865\n",
            "Epoch: 029, Loss: 0.62086, Train Acc: 0.65569, Test Acc: 0.64865\n",
            "Epoch: 030, Loss: 0.62150, Train Acc: 0.65569, Test Acc: 0.64865\n",
            "Epoch: 031, Loss: 0.62236, Train Acc: 0.65669, Test Acc: 0.64865\n",
            "Epoch: 032, Loss: 0.61695, Train Acc: 0.65868, Test Acc: 0.64865\n",
            "Epoch: 033, Loss: 0.61796, Train Acc: 0.66068, Test Acc: 0.65766\n",
            "Epoch: 034, Loss: 0.61992, Train Acc: 0.66267, Test Acc: 0.65766\n",
            "Epoch: 035, Loss: 0.61936, Train Acc: 0.66267, Test Acc: 0.65766\n",
            "Epoch: 036, Loss: 0.62156, Train Acc: 0.66068, Test Acc: 0.65766\n",
            "Epoch: 037, Loss: 0.61865, Train Acc: 0.66267, Test Acc: 0.65766\n",
            "Epoch: 038, Loss: 0.61717, Train Acc: 0.66467, Test Acc: 0.67568\n",
            "Epoch: 039, Loss: 0.62022, Train Acc: 0.66367, Test Acc: 0.67568\n",
            "Epoch: 040, Loss: 0.61659, Train Acc: 0.66367, Test Acc: 0.68468\n",
            "Epoch: 041, Loss: 0.62125, Train Acc: 0.66467, Test Acc: 0.67568\n",
            "Epoch: 042, Loss: 0.62131, Train Acc: 0.66467, Test Acc: 0.67568\n",
            "Epoch: 043, Loss: 0.61995, Train Acc: 0.66467, Test Acc: 0.67568\n",
            "Epoch: 044, Loss: 0.61613, Train Acc: 0.66567, Test Acc: 0.67568\n",
            "Epoch: 045, Loss: 0.61950, Train Acc: 0.66467, Test Acc: 0.68468\n",
            "Epoch: 046, Loss: 0.61818, Train Acc: 0.66467, Test Acc: 0.68468\n",
            "Epoch: 047, Loss: 0.62057, Train Acc: 0.66667, Test Acc: 0.68468\n",
            "Epoch: 048, Loss: 0.62150, Train Acc: 0.66766, Test Acc: 0.68468\n",
            "Epoch: 049, Loss: 0.61748, Train Acc: 0.66766, Test Acc: 0.68468\n",
            "Epoch: 050, Loss: 0.61423, Train Acc: 0.66766, Test Acc: 0.67568\n",
            "Epoch: 051, Loss: 0.61731, Train Acc: 0.66866, Test Acc: 0.67568\n",
            "Epoch: 052, Loss: 0.62093, Train Acc: 0.66866, Test Acc: 0.67568\n",
            "Epoch: 053, Loss: 0.61588, Train Acc: 0.66866, Test Acc: 0.67568\n",
            "Epoch: 054, Loss: 0.61634, Train Acc: 0.66966, Test Acc: 0.67568\n",
            "Epoch: 055, Loss: 0.61350, Train Acc: 0.66866, Test Acc: 0.67568\n",
            "Epoch: 056, Loss: 0.61486, Train Acc: 0.66667, Test Acc: 0.67568\n",
            "Epoch: 057, Loss: 0.61490, Train Acc: 0.66567, Test Acc: 0.68468\n",
            "Epoch: 058, Loss: 0.61813, Train Acc: 0.66467, Test Acc: 0.68468\n",
            "Epoch: 059, Loss: 0.61440, Train Acc: 0.66567, Test Acc: 0.68468\n",
            "Epoch: 060, Loss: 0.61586, Train Acc: 0.66467, Test Acc: 0.68468\n",
            "Epoch: 061, Loss: 0.61551, Train Acc: 0.66567, Test Acc: 0.68468\n",
            "Epoch: 062, Loss: 0.61738, Train Acc: 0.66766, Test Acc: 0.68468\n",
            "Epoch: 063, Loss: 0.61372, Train Acc: 0.66667, Test Acc: 0.68468\n",
            "Epoch: 064, Loss: 0.61302, Train Acc: 0.66467, Test Acc: 0.67568\n",
            "Epoch: 065, Loss: 0.61770, Train Acc: 0.66766, Test Acc: 0.68468\n",
            "Epoch: 066, Loss: 0.61379, Train Acc: 0.66766, Test Acc: 0.68468\n",
            "Epoch: 067, Loss: 0.61424, Train Acc: 0.66766, Test Acc: 0.68468\n",
            "Epoch: 068, Loss: 0.61619, Train Acc: 0.66766, Test Acc: 0.68468\n",
            "Epoch: 069, Loss: 0.61546, Train Acc: 0.66766, Test Acc: 0.68468\n",
            "Epoch: 070, Loss: 0.61824, Train Acc: 0.66866, Test Acc: 0.68468\n",
            "Epoch: 071, Loss: 0.61486, Train Acc: 0.66866, Test Acc: 0.68468\n",
            "Epoch: 072, Loss: 0.61301, Train Acc: 0.66866, Test Acc: 0.68468\n",
            "Epoch: 073, Loss: 0.61497, Train Acc: 0.66866, Test Acc: 0.68468\n",
            "Epoch: 074, Loss: 0.61203, Train Acc: 0.66567, Test Acc: 0.68468\n",
            "Epoch: 075, Loss: 0.61533, Train Acc: 0.66866, Test Acc: 0.69369\n",
            "Epoch: 076, Loss: 0.61737, Train Acc: 0.66567, Test Acc: 0.68468\n",
            "Epoch: 077, Loss: 0.61479, Train Acc: 0.66866, Test Acc: 0.69369\n",
            "Epoch: 078, Loss: 0.61724, Train Acc: 0.66866, Test Acc: 0.69369\n",
            "Epoch: 079, Loss: 0.61433, Train Acc: 0.66966, Test Acc: 0.69369\n",
            "Epoch: 080, Loss: 0.61261, Train Acc: 0.66667, Test Acc: 0.69369\n",
            "Epoch: 081, Loss: 0.61598, Train Acc: 0.66168, Test Acc: 0.68468\n",
            "Epoch: 082, Loss: 0.61905, Train Acc: 0.66866, Test Acc: 0.69369\n",
            "Epoch: 083, Loss: 0.61533, Train Acc: 0.66866, Test Acc: 0.69369\n",
            "Epoch: 084, Loss: 0.61407, Train Acc: 0.66667, Test Acc: 0.69369\n",
            "Epoch: 085, Loss: 0.60824, Train Acc: 0.66367, Test Acc: 0.69369\n",
            "Epoch: 086, Loss: 0.61353, Train Acc: 0.66467, Test Acc: 0.68468\n",
            "Epoch: 087, Loss: 0.61264, Train Acc: 0.66367, Test Acc: 0.68468\n",
            "Epoch: 088, Loss: 0.61604, Train Acc: 0.66367, Test Acc: 0.69369\n",
            "Epoch: 089, Loss: 0.61301, Train Acc: 0.66267, Test Acc: 0.69369\n",
            "Epoch: 090, Loss: 0.61490, Train Acc: 0.66367, Test Acc: 0.68468\n",
            "Epoch: 091, Loss: 0.61292, Train Acc: 0.66367, Test Acc: 0.69369\n",
            "Epoch: 092, Loss: 0.61463, Train Acc: 0.66267, Test Acc: 0.69369\n",
            "Epoch: 093, Loss: 0.61716, Train Acc: 0.66267, Test Acc: 0.69369\n",
            "Epoch: 094, Loss: 0.61245, Train Acc: 0.66667, Test Acc: 0.69369\n",
            "Epoch: 095, Loss: 0.61116, Train Acc: 0.66467, Test Acc: 0.69369\n",
            "Epoch: 096, Loss: 0.61025, Train Acc: 0.66367, Test Acc: 0.69369\n",
            "Epoch: 097, Loss: 0.61120, Train Acc: 0.66367, Test Acc: 0.69369\n",
            "Epoch: 098, Loss: 0.61124, Train Acc: 0.66367, Test Acc: 0.69369\n",
            "Epoch: 099, Loss: 0.61110, Train Acc: 0.66367, Test Acc: 0.68468\n",
            "Epoch: 100, Loss: 0.61257, Train Acc: 0.66467, Test Acc: 0.69369\n",
            "Epoch: 101, Loss: 0.61502, Train Acc: 0.66367, Test Acc: 0.69369\n",
            "Epoch: 102, Loss: 0.61572, Train Acc: 0.66367, Test Acc: 0.69369\n",
            "Epoch: 103, Loss: 0.61710, Train Acc: 0.66667, Test Acc: 0.69369\n",
            "Epoch: 104, Loss: 0.61243, Train Acc: 0.66467, Test Acc: 0.69369\n",
            "Epoch: 105, Loss: 0.61612, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 106, Loss: 0.61579, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 107, Loss: 0.61007, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 108, Loss: 0.61430, Train Acc: 0.66467, Test Acc: 0.70270\n",
            "Epoch: 109, Loss: 0.61276, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 110, Loss: 0.61395, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 111, Loss: 0.61876, Train Acc: 0.65968, Test Acc: 0.69369\n",
            "Epoch: 112, Loss: 0.61043, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 113, Loss: 0.61326, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 114, Loss: 0.61072, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 115, Loss: 0.61178, Train Acc: 0.65968, Test Acc: 0.69369\n",
            "Epoch: 116, Loss: 0.61661, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 117, Loss: 0.61720, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 118, Loss: 0.61259, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 119, Loss: 0.61327, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 120, Loss: 0.61508, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 121, Loss: 0.61493, Train Acc: 0.65868, Test Acc: 0.69369\n",
            "Epoch: 122, Loss: 0.60987, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 123, Loss: 0.61490, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 124, Loss: 0.61644, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 125, Loss: 0.61063, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 126, Loss: 0.61366, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 127, Loss: 0.61214, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 128, Loss: 0.60771, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 129, Loss: 0.60846, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 130, Loss: 0.61420, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 131, Loss: 0.60919, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 132, Loss: 0.60929, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 133, Loss: 0.61286, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 134, Loss: 0.61155, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 135, Loss: 0.61059, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 136, Loss: 0.61197, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 137, Loss: 0.60833, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 138, Loss: 0.61416, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 139, Loss: 0.61145, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 140, Loss: 0.61162, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 141, Loss: 0.61150, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 142, Loss: 0.61181, Train Acc: 0.65968, Test Acc: 0.70270\n",
            "Epoch: 143, Loss: 0.61434, Train Acc: 0.66068, Test Acc: 0.69369\n",
            "Epoch: 144, Loss: 0.61367, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 145, Loss: 0.60868, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 146, Loss: 0.61371, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 147, Loss: 0.61077, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 148, Loss: 0.60707, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 149, Loss: 0.60875, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 150, Loss: 0.61036, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 151, Loss: 0.61027, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 152, Loss: 0.61126, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 153, Loss: 0.60776, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 154, Loss: 0.61370, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 155, Loss: 0.61020, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 156, Loss: 0.61045, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 157, Loss: 0.60969, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 158, Loss: 0.61311, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 159, Loss: 0.61465, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 160, Loss: 0.60691, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 161, Loss: 0.60991, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 162, Loss: 0.61112, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 163, Loss: 0.61277, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 164, Loss: 0.60874, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 165, Loss: 0.60587, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 166, Loss: 0.60934, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 167, Loss: 0.60919, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 168, Loss: 0.61067, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 169, Loss: 0.61366, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 170, Loss: 0.61263, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 171, Loss: 0.61128, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 172, Loss: 0.60608, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 173, Loss: 0.60834, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 174, Loss: 0.60904, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 175, Loss: 0.61175, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 176, Loss: 0.61464, Train Acc: 0.65968, Test Acc: 0.70270\n",
            "Epoch: 177, Loss: 0.60771, Train Acc: 0.65868, Test Acc: 0.70270\n",
            "Epoch: 178, Loss: 0.61058, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 179, Loss: 0.60705, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 180, Loss: 0.60766, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 181, Loss: 0.61047, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 182, Loss: 0.60617, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 183, Loss: 0.61014, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 184, Loss: 0.60950, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 185, Loss: 0.60767, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 186, Loss: 0.61010, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 187, Loss: 0.60358, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 188, Loss: 0.60761, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 189, Loss: 0.61412, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 190, Loss: 0.61105, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 191, Loss: 0.60559, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 192, Loss: 0.61063, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 193, Loss: 0.60979, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 194, Loss: 0.60866, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 195, Loss: 0.60947, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 196, Loss: 0.61142, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 197, Loss: 0.61059, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 198, Loss: 0.60546, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 199, Loss: 0.61089, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 200, Loss: 0.60691, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 201, Loss: 0.60887, Train Acc: 0.66168, Test Acc: 0.71171\n",
            "Epoch: 202, Loss: 0.61222, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 203, Loss: 0.61070, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 204, Loss: 0.60848, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 205, Loss: 0.60739, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 206, Loss: 0.61065, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 207, Loss: 0.60633, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 208, Loss: 0.60941, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 209, Loss: 0.60995, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 210, Loss: 0.60850, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 211, Loss: 0.60874, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 212, Loss: 0.60612, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 213, Loss: 0.60965, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 214, Loss: 0.60508, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 215, Loss: 0.60913, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 216, Loss: 0.60557, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 217, Loss: 0.61004, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 218, Loss: 0.61052, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 219, Loss: 0.60659, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 220, Loss: 0.60712, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 221, Loss: 0.60893, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 222, Loss: 0.60653, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 223, Loss: 0.60827, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 224, Loss: 0.61270, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 225, Loss: 0.61159, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 226, Loss: 0.60980, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 227, Loss: 0.60080, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 228, Loss: 0.60406, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 229, Loss: 0.60550, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 230, Loss: 0.60752, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 231, Loss: 0.60399, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 232, Loss: 0.60731, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 233, Loss: 0.60601, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 234, Loss: 0.60569, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 235, Loss: 0.61070, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 236, Loss: 0.60621, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 237, Loss: 0.60819, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 238, Loss: 0.60070, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 239, Loss: 0.60666, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 240, Loss: 0.60821, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 241, Loss: 0.60712, Train Acc: 0.66267, Test Acc: 0.71171\n",
            "Epoch: 242, Loss: 0.60714, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 243, Loss: 0.60758, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 244, Loss: 0.60627, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 245, Loss: 0.61135, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 246, Loss: 0.60752, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 247, Loss: 0.60883, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 248, Loss: 0.60605, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 249, Loss: 0.60957, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 250, Loss: 0.60415, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 251, Loss: 0.60836, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 252, Loss: 0.61008, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 253, Loss: 0.60871, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 254, Loss: 0.60542, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 255, Loss: 0.60700, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 256, Loss: 0.59962, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 257, Loss: 0.60327, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 258, Loss: 0.60947, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 259, Loss: 0.60689, Train Acc: 0.66168, Test Acc: 0.71171\n",
            "Epoch: 260, Loss: 0.60866, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 261, Loss: 0.60139, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 262, Loss: 0.60030, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 263, Loss: 0.60192, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 264, Loss: 0.60916, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 265, Loss: 0.60709, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 266, Loss: 0.60236, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 267, Loss: 0.60615, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 268, Loss: 0.60797, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 269, Loss: 0.59755, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 270, Loss: 0.60421, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 271, Loss: 0.60331, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 272, Loss: 0.60518, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 273, Loss: 0.60998, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 274, Loss: 0.60612, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 275, Loss: 0.60536, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 276, Loss: 0.60813, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 277, Loss: 0.60055, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 278, Loss: 0.60454, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 279, Loss: 0.60500, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 280, Loss: 0.61284, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 281, Loss: 0.60844, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 282, Loss: 0.61083, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 283, Loss: 0.60850, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 284, Loss: 0.60386, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 285, Loss: 0.60391, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 286, Loss: 0.60654, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 287, Loss: 0.61230, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 288, Loss: 0.59863, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 289, Loss: 0.60421, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 290, Loss: 0.60367, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 291, Loss: 0.60134, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 292, Loss: 0.60631, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 293, Loss: 0.60667, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 294, Loss: 0.60897, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 295, Loss: 0.60629, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 296, Loss: 0.60390, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 297, Loss: 0.61063, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 298, Loss: 0.60044, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 299, Loss: 0.60321, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 300, Loss: 0.60815, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 301, Loss: 0.60079, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 302, Loss: 0.60387, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 303, Loss: 0.59942, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 304, Loss: 0.60628, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 305, Loss: 0.60661, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 306, Loss: 0.60897, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 307, Loss: 0.60457, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 308, Loss: 0.60280, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 309, Loss: 0.59808, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 310, Loss: 0.60512, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 311, Loss: 0.60893, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 312, Loss: 0.60583, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 313, Loss: 0.60901, Train Acc: 0.66267, Test Acc: 0.71171\n",
            "Epoch: 314, Loss: 0.60283, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 315, Loss: 0.60634, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 316, Loss: 0.60714, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 317, Loss: 0.60327, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 318, Loss: 0.60065, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 319, Loss: 0.60560, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 320, Loss: 0.60071, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 321, Loss: 0.60989, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 322, Loss: 0.60107, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 323, Loss: 0.60567, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 324, Loss: 0.60071, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 325, Loss: 0.61084, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 326, Loss: 0.60976, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 327, Loss: 0.60518, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 328, Loss: 0.60676, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 329, Loss: 0.60713, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 330, Loss: 0.60299, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 331, Loss: 0.60591, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 332, Loss: 0.61009, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 333, Loss: 0.60758, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 334, Loss: 0.60246, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 335, Loss: 0.61139, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 336, Loss: 0.60645, Train Acc: 0.66068, Test Acc: 0.70270\n",
            "Epoch: 337, Loss: 0.60471, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 338, Loss: 0.60742, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 339, Loss: 0.59847, Train Acc: 0.66367, Test Acc: 0.71171\n",
            "Epoch: 340, Loss: 0.60233, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 341, Loss: 0.60293, Train Acc: 0.66367, Test Acc: 0.71171\n",
            "Epoch: 342, Loss: 0.60287, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 343, Loss: 0.60777, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 344, Loss: 0.60628, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 345, Loss: 0.61035, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 346, Loss: 0.60514, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 347, Loss: 0.60419, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 348, Loss: 0.60352, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 349, Loss: 0.59863, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 350, Loss: 0.60619, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 351, Loss: 0.60353, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 352, Loss: 0.60185, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 353, Loss: 0.60892, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 354, Loss: 0.60420, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 355, Loss: 0.60216, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 356, Loss: 0.60010, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 357, Loss: 0.60463, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 358, Loss: 0.60707, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 359, Loss: 0.60586, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 360, Loss: 0.61100, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 361, Loss: 0.60815, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 362, Loss: 0.60082, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 363, Loss: 0.59734, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 364, Loss: 0.60285, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 365, Loss: 0.60541, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 366, Loss: 0.60369, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 367, Loss: 0.60394, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 368, Loss: 0.60185, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 369, Loss: 0.60260, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 370, Loss: 0.60305, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 371, Loss: 0.60362, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 372, Loss: 0.60435, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 373, Loss: 0.60397, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 374, Loss: 0.60153, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 375, Loss: 0.60579, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 376, Loss: 0.60948, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 377, Loss: 0.60468, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 378, Loss: 0.60566, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 379, Loss: 0.61021, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 380, Loss: 0.59690, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 381, Loss: 0.60073, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 382, Loss: 0.60852, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 383, Loss: 0.60380, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 384, Loss: 0.60593, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 385, Loss: 0.60030, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 386, Loss: 0.60412, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 387, Loss: 0.60456, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 388, Loss: 0.60336, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 389, Loss: 0.60485, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 390, Loss: 0.60233, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 391, Loss: 0.60245, Train Acc: 0.66168, Test Acc: 0.72072\n",
            "Epoch: 392, Loss: 0.59838, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 393, Loss: 0.60113, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 394, Loss: 0.60234, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 395, Loss: 0.60232, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 396, Loss: 0.60116, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 397, Loss: 0.60569, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 398, Loss: 0.60091, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 399, Loss: 0.59815, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 400, Loss: 0.60451, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 401, Loss: 0.59808, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 402, Loss: 0.60186, Train Acc: 0.66168, Test Acc: 0.71171\n",
            "Epoch: 403, Loss: 0.61259, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 404, Loss: 0.60465, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 405, Loss: 0.60304, Train Acc: 0.66866, Test Acc: 0.72072\n",
            "Epoch: 406, Loss: 0.60274, Train Acc: 0.66667, Test Acc: 0.70270\n",
            "Epoch: 407, Loss: 0.60256, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 408, Loss: 0.60363, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 409, Loss: 0.60647, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 410, Loss: 0.60568, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 411, Loss: 0.60249, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 412, Loss: 0.60439, Train Acc: 0.66667, Test Acc: 0.71171\n",
            "Epoch: 413, Loss: 0.60206, Train Acc: 0.66866, Test Acc: 0.72072\n",
            "Epoch: 414, Loss: 0.60762, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 415, Loss: 0.60396, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 416, Loss: 0.60442, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 417, Loss: 0.60420, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 418, Loss: 0.60803, Train Acc: 0.66267, Test Acc: 0.70270\n",
            "Epoch: 419, Loss: 0.60405, Train Acc: 0.66766, Test Acc: 0.72072\n",
            "Epoch: 420, Loss: 0.60703, Train Acc: 0.66766, Test Acc: 0.72072\n",
            "Epoch: 421, Loss: 0.60261, Train Acc: 0.66367, Test Acc: 0.70270\n",
            "Epoch: 422, Loss: 0.60790, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 423, Loss: 0.60187, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 424, Loss: 0.60564, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 425, Loss: 0.60797, Train Acc: 0.66766, Test Acc: 0.72072\n",
            "Epoch: 426, Loss: 0.59916, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 427, Loss: 0.60883, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 428, Loss: 0.59934, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 429, Loss: 0.60003, Train Acc: 0.66168, Test Acc: 0.70270\n",
            "Epoch: 430, Loss: 0.60536, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 431, Loss: 0.59969, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 432, Loss: 0.60186, Train Acc: 0.66467, Test Acc: 0.72072\n",
            "Epoch: 433, Loss: 0.60589, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 434, Loss: 0.60328, Train Acc: 0.66567, Test Acc: 0.71171\n",
            "Epoch: 435, Loss: 0.61147, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 436, Loss: 0.60234, Train Acc: 0.66667, Test Acc: 0.70270\n",
            "Epoch: 437, Loss: 0.60405, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 438, Loss: 0.60180, Train Acc: 0.66866, Test Acc: 0.72072\n",
            "Epoch: 439, Loss: 0.60254, Train Acc: 0.66766, Test Acc: 0.72072\n",
            "Epoch: 440, Loss: 0.60291, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 441, Loss: 0.60405, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 442, Loss: 0.60239, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 443, Loss: 0.60395, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 444, Loss: 0.60163, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 445, Loss: 0.60348, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 446, Loss: 0.60081, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 447, Loss: 0.59993, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 448, Loss: 0.59715, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 449, Loss: 0.60510, Train Acc: 0.66766, Test Acc: 0.72072\n",
            "Epoch: 450, Loss: 0.59935, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 451, Loss: 0.60188, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 452, Loss: 0.60932, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 453, Loss: 0.59862, Train Acc: 0.66367, Test Acc: 0.72072\n",
            "Epoch: 454, Loss: 0.60374, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 455, Loss: 0.60662, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 456, Loss: 0.60587, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 457, Loss: 0.60205, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 458, Loss: 0.60505, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 459, Loss: 0.60276, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 460, Loss: 0.60271, Train Acc: 0.66267, Test Acc: 0.72072\n",
            "Epoch: 461, Loss: 0.60518, Train Acc: 0.66766, Test Acc: 0.72072\n",
            "Epoch: 462, Loss: 0.60110, Train Acc: 0.66467, Test Acc: 0.71171\n",
            "Epoch: 463, Loss: 0.60376, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 464, Loss: 0.60532, Train Acc: 0.66866, Test Acc: 0.72072\n",
            "Epoch: 465, Loss: 0.60240, Train Acc: 0.66866, Test Acc: 0.72072\n",
            "Epoch: 466, Loss: 0.60685, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 467, Loss: 0.60362, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 468, Loss: 0.60147, Train Acc: 0.66766, Test Acc: 0.72072\n",
            "Epoch: 469, Loss: 0.60626, Train Acc: 0.66667, Test Acc: 0.70270\n",
            "Epoch: 470, Loss: 0.60147, Train Acc: 0.66567, Test Acc: 0.71171\n",
            "Epoch: 471, Loss: 0.60587, Train Acc: 0.66766, Test Acc: 0.72072\n",
            "Epoch: 472, Loss: 0.60334, Train Acc: 0.66667, Test Acc: 0.70270\n",
            "Epoch: 473, Loss: 0.60685, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 474, Loss: 0.60764, Train Acc: 0.66667, Test Acc: 0.70270\n",
            "Epoch: 475, Loss: 0.60586, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 476, Loss: 0.59789, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 477, Loss: 0.60283, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 478, Loss: 0.60489, Train Acc: 0.66866, Test Acc: 0.72072\n",
            "Epoch: 479, Loss: 0.60845, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 480, Loss: 0.60275, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 481, Loss: 0.60594, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 482, Loss: 0.60253, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 483, Loss: 0.60560, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 484, Loss: 0.60175, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 485, Loss: 0.60360, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 486, Loss: 0.60083, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 487, Loss: 0.60372, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 488, Loss: 0.60180, Train Acc: 0.66467, Test Acc: 0.70270\n",
            "Epoch: 489, Loss: 0.59670, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 490, Loss: 0.60512, Train Acc: 0.66567, Test Acc: 0.70270\n",
            "Epoch: 491, Loss: 0.60376, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 492, Loss: 0.60128, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 493, Loss: 0.60484, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 494, Loss: 0.60378, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 495, Loss: 0.60504, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 496, Loss: 0.60264, Train Acc: 0.66567, Test Acc: 0.72072\n",
            "Epoch: 497, Loss: 0.60532, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 498, Loss: 0.60474, Train Acc: 0.66667, Test Acc: 0.72072\n",
            "Epoch: 499, Loss: 0.60591, Train Acc: 0.66766, Test Acc: 0.72072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV1S3ReRqq0X"
      },
      "source": [
        "#### Enzymes dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zbOe3ebf_4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3009b44-88c1-4b46-99b2-842202765dce"
      },
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, SAGEConv, BatchNorm\n",
        "import torch.nn.functional as f\n",
        "\n",
        "#https://github.com/rusty1s/pytorch_geometric/blob/cff934ec28fb70d05d434c74111047078667ace8/examples/proteins_topk_pool.py#L25\n",
        "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "n = len(dataset) // 10\n",
        "b_size = 32\n",
        "test_dataset = dataset[:n]\n",
        "train_dataset = dataset[n:]\n",
        "test_loader = DataLoader(test_dataset, batch_size=b_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=b_size)\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    \n",
        "    self.bnornm1 = BatchNorm(dataset.num_node_features)\n",
        "    self.bnornm2 = BatchNorm(128)\n",
        "    self.bnornm3 = BatchNorm(128)\n",
        "\n",
        "    self.conv1 = SAGEConv(dataset.num_node_features, 128, normalize=True)\n",
        "    self.conv2 = SAGEConv(128, 128, normalize=True)\n",
        "    self.pool  = CliquePooling('avg')\n",
        "    self.conv3 = SAGEConv(128, dataset.num_classes, normalize=True)\n",
        "\n",
        "  def forward(self, data):\n",
        "    ## 1. Obtain node embeddings:\n",
        "    x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "    x = self.bnornm1(x)\n",
        "    # First GCN\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    # x = F.dropout(x, training=self.training)\n",
        "    # Second GCN\n",
        "    x = self.bnornm2(x)\n",
        "    x = self.conv2(x, edge_index) \n",
        "    x = F.relu(x)\n",
        "    # x = F.dropout(x, training=self.training)\n",
        "    x, d_edge_index, _, d_batch, _ = self.pool(x, edge_index, None, batch)\n",
        "    x = F.relu(x)\n",
        "    # x = F.dropout(x, training=self.training)\n",
        "    # Third GCN\n",
        "    x = self.bnornm3(x)\n",
        "    x = self.conv3(x, d_edge_index)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    ## 2. Readout layer:\n",
        "    x = global_mean_pool(x, d_batch)\n",
        "\n",
        "    x = F.log_softmax(x, dim=-1)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Extracting /tmp/ENZYMES/ENZYMES/ENZYMES.zip\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QP35lAbpf_4P",
        "outputId": "c5aeac2f-2698-462a-ce43-cb1d27c6c534"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001) # lr=0.01, weight_decay=5e-4\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "\n",
        "  loss_all = 0\n",
        "  for data in train_loader:\n",
        "    data = data.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, data.y)\n",
        "    loss.backward()\n",
        "    loss_all += data.num_graphs * loss.item()\n",
        "    optimizer.step()\n",
        "  return loss_all / len(train_dataset)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  for data in loader:\n",
        "    data = data.to(device)\n",
        "    pred = model(data).max(dim=1)[1]\n",
        "    correct += pred.eq(data.y).sum().item()\n",
        "  return correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "for epoch in range(1, 1000):\n",
        "  loss = train(epoch)\n",
        "  train_acc = test(train_loader)\n",
        "  test_acc = test(test_loader)\n",
        "  print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.\n",
        "        format(epoch, loss, train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 1.77248, Train Acc: 0.21852, Test Acc: 0.23333\n",
            "Epoch: 002, Loss: 1.75264, Train Acc: 0.20370, Test Acc: 0.23333\n",
            "Epoch: 003, Loss: 1.74269, Train Acc: 0.25926, Test Acc: 0.25000\n",
            "Epoch: 004, Loss: 1.73432, Train Acc: 0.29815, Test Acc: 0.28333\n",
            "Epoch: 005, Loss: 1.72698, Train Acc: 0.30741, Test Acc: 0.30000\n",
            "Epoch: 006, Loss: 1.72083, Train Acc: 0.30185, Test Acc: 0.33333\n",
            "Epoch: 007, Loss: 1.71562, Train Acc: 0.30741, Test Acc: 0.31667\n",
            "Epoch: 008, Loss: 1.71151, Train Acc: 0.31667, Test Acc: 0.30000\n",
            "Epoch: 009, Loss: 1.70724, Train Acc: 0.32037, Test Acc: 0.30000\n",
            "Epoch: 010, Loss: 1.70342, Train Acc: 0.32593, Test Acc: 0.30000\n",
            "Epoch: 011, Loss: 1.69930, Train Acc: 0.33333, Test Acc: 0.28333\n",
            "Epoch: 012, Loss: 1.69578, Train Acc: 0.33889, Test Acc: 0.28333\n",
            "Epoch: 013, Loss: 1.69290, Train Acc: 0.33519, Test Acc: 0.28333\n",
            "Epoch: 014, Loss: 1.68998, Train Acc: 0.33889, Test Acc: 0.30000\n",
            "Epoch: 015, Loss: 1.68740, Train Acc: 0.34815, Test Acc: 0.30000\n",
            "Epoch: 016, Loss: 1.68469, Train Acc: 0.35370, Test Acc: 0.28333\n",
            "Epoch: 017, Loss: 1.68263, Train Acc: 0.35741, Test Acc: 0.30000\n",
            "Epoch: 018, Loss: 1.67995, Train Acc: 0.36667, Test Acc: 0.30000\n",
            "Epoch: 019, Loss: 1.67785, Train Acc: 0.37778, Test Acc: 0.28333\n",
            "Epoch: 020, Loss: 1.67571, Train Acc: 0.37593, Test Acc: 0.26667\n",
            "Epoch: 021, Loss: 1.67354, Train Acc: 0.38148, Test Acc: 0.28333\n",
            "Epoch: 022, Loss: 1.67181, Train Acc: 0.37963, Test Acc: 0.28333\n",
            "Epoch: 023, Loss: 1.66986, Train Acc: 0.38519, Test Acc: 0.26667\n",
            "Epoch: 024, Loss: 1.66827, Train Acc: 0.38889, Test Acc: 0.26667\n",
            "Epoch: 025, Loss: 1.66636, Train Acc: 0.39259, Test Acc: 0.26667\n",
            "Epoch: 026, Loss: 1.66503, Train Acc: 0.39074, Test Acc: 0.26667\n",
            "Epoch: 027, Loss: 1.66343, Train Acc: 0.39074, Test Acc: 0.26667\n",
            "Epoch: 028, Loss: 1.66172, Train Acc: 0.39259, Test Acc: 0.26667\n",
            "Epoch: 029, Loss: 1.66011, Train Acc: 0.39815, Test Acc: 0.26667\n",
            "Epoch: 030, Loss: 1.65864, Train Acc: 0.39444, Test Acc: 0.26667\n",
            "Epoch: 031, Loss: 1.65679, Train Acc: 0.39630, Test Acc: 0.26667\n",
            "Epoch: 032, Loss: 1.65516, Train Acc: 0.39815, Test Acc: 0.26667\n",
            "Epoch: 033, Loss: 1.65371, Train Acc: 0.40000, Test Acc: 0.26667\n",
            "Epoch: 034, Loss: 1.65217, Train Acc: 0.39444, Test Acc: 0.26667\n",
            "Epoch: 035, Loss: 1.65042, Train Acc: 0.40000, Test Acc: 0.28333\n",
            "Epoch: 036, Loss: 1.64870, Train Acc: 0.40185, Test Acc: 0.26667\n",
            "Epoch: 037, Loss: 1.64707, Train Acc: 0.39815, Test Acc: 0.28333\n",
            "Epoch: 038, Loss: 1.64556, Train Acc: 0.39630, Test Acc: 0.28333\n",
            "Epoch: 039, Loss: 1.64436, Train Acc: 0.40556, Test Acc: 0.28333\n",
            "Epoch: 040, Loss: 1.64286, Train Acc: 0.41296, Test Acc: 0.28333\n",
            "Epoch: 041, Loss: 1.64169, Train Acc: 0.41296, Test Acc: 0.28333\n",
            "Epoch: 042, Loss: 1.64005, Train Acc: 0.40741, Test Acc: 0.28333\n",
            "Epoch: 043, Loss: 1.63891, Train Acc: 0.40741, Test Acc: 0.30000\n",
            "Epoch: 044, Loss: 1.63743, Train Acc: 0.40000, Test Acc: 0.30000\n",
            "Epoch: 045, Loss: 1.63632, Train Acc: 0.40370, Test Acc: 0.31667\n",
            "Epoch: 046, Loss: 1.63487, Train Acc: 0.40556, Test Acc: 0.31667\n",
            "Epoch: 047, Loss: 1.63334, Train Acc: 0.40741, Test Acc: 0.33333\n",
            "Epoch: 048, Loss: 1.63178, Train Acc: 0.40556, Test Acc: 0.31667\n",
            "Epoch: 049, Loss: 1.63058, Train Acc: 0.40000, Test Acc: 0.31667\n",
            "Epoch: 050, Loss: 1.62927, Train Acc: 0.40000, Test Acc: 0.33333\n",
            "Epoch: 051, Loss: 1.62823, Train Acc: 0.39259, Test Acc: 0.35000\n",
            "Epoch: 052, Loss: 1.62709, Train Acc: 0.38889, Test Acc: 0.33333\n",
            "Epoch: 053, Loss: 1.62646, Train Acc: 0.38519, Test Acc: 0.31667\n",
            "Epoch: 054, Loss: 1.62541, Train Acc: 0.39074, Test Acc: 0.30000\n",
            "Epoch: 055, Loss: 1.62465, Train Acc: 0.38519, Test Acc: 0.33333\n",
            "Epoch: 056, Loss: 1.62278, Train Acc: 0.37963, Test Acc: 0.33333\n",
            "Epoch: 057, Loss: 1.62084, Train Acc: 0.39444, Test Acc: 0.33333\n",
            "Epoch: 058, Loss: 1.61935, Train Acc: 0.38889, Test Acc: 0.33333\n",
            "Epoch: 059, Loss: 1.61751, Train Acc: 0.39815, Test Acc: 0.33333\n",
            "Epoch: 060, Loss: 1.61642, Train Acc: 0.39444, Test Acc: 0.30000\n",
            "Epoch: 061, Loss: 1.61519, Train Acc: 0.41111, Test Acc: 0.33333\n",
            "Epoch: 062, Loss: 1.61451, Train Acc: 0.39815, Test Acc: 0.30000\n",
            "Epoch: 063, Loss: 1.61379, Train Acc: 0.39444, Test Acc: 0.30000\n",
            "Epoch: 064, Loss: 1.61334, Train Acc: 0.40370, Test Acc: 0.28333\n",
            "Epoch: 065, Loss: 1.61278, Train Acc: 0.39630, Test Acc: 0.26667\n",
            "Epoch: 066, Loss: 1.61133, Train Acc: 0.39259, Test Acc: 0.28333\n",
            "Epoch: 067, Loss: 1.60968, Train Acc: 0.40370, Test Acc: 0.30000\n",
            "Epoch: 068, Loss: 1.60880, Train Acc: 0.39630, Test Acc: 0.30000\n",
            "Epoch: 069, Loss: 1.60790, Train Acc: 0.39259, Test Acc: 0.35000\n",
            "Epoch: 070, Loss: 1.60837, Train Acc: 0.40185, Test Acc: 0.36667\n",
            "Epoch: 071, Loss: 1.60732, Train Acc: 0.40741, Test Acc: 0.38333\n",
            "Epoch: 072, Loss: 1.60590, Train Acc: 0.41296, Test Acc: 0.35000\n",
            "Epoch: 073, Loss: 1.60446, Train Acc: 0.39444, Test Acc: 0.35000\n",
            "Epoch: 074, Loss: 1.60467, Train Acc: 0.40185, Test Acc: 0.35000\n",
            "Epoch: 075, Loss: 1.60469, Train Acc: 0.39259, Test Acc: 0.31667\n",
            "Epoch: 076, Loss: 1.60418, Train Acc: 0.39630, Test Acc: 0.31667\n",
            "Epoch: 077, Loss: 1.60208, Train Acc: 0.40000, Test Acc: 0.30000\n",
            "Epoch: 078, Loss: 1.59960, Train Acc: 0.39815, Test Acc: 0.30000\n",
            "Epoch: 079, Loss: 1.59807, Train Acc: 0.38704, Test Acc: 0.28333\n",
            "Epoch: 080, Loss: 1.59812, Train Acc: 0.38148, Test Acc: 0.28333\n",
            "Epoch: 081, Loss: 1.59980, Train Acc: 0.37963, Test Acc: 0.31667\n",
            "Epoch: 082, Loss: 1.60119, Train Acc: 0.37963, Test Acc: 0.33333\n",
            "Epoch: 083, Loss: 1.59911, Train Acc: 0.39074, Test Acc: 0.31667\n",
            "Epoch: 084, Loss: 1.59681, Train Acc: 0.39074, Test Acc: 0.31667\n",
            "Epoch: 085, Loss: 1.59496, Train Acc: 0.40556, Test Acc: 0.31667\n",
            "Epoch: 086, Loss: 1.59546, Train Acc: 0.38889, Test Acc: 0.31667\n",
            "Epoch: 087, Loss: 1.59469, Train Acc: 0.37963, Test Acc: 0.33333\n",
            "Epoch: 088, Loss: 1.59374, Train Acc: 0.38519, Test Acc: 0.38333\n",
            "Epoch: 089, Loss: 1.59258, Train Acc: 0.38519, Test Acc: 0.38333\n",
            "Epoch: 090, Loss: 1.59055, Train Acc: 0.38333, Test Acc: 0.35000\n",
            "Epoch: 091, Loss: 1.58873, Train Acc: 0.39815, Test Acc: 0.36667\n",
            "Epoch: 092, Loss: 1.58728, Train Acc: 0.39259, Test Acc: 0.36667\n",
            "Epoch: 093, Loss: 1.58604, Train Acc: 0.39815, Test Acc: 0.35000\n",
            "Epoch: 094, Loss: 1.58440, Train Acc: 0.39444, Test Acc: 0.33333\n",
            "Epoch: 095, Loss: 1.58328, Train Acc: 0.40556, Test Acc: 0.33333\n",
            "Epoch: 096, Loss: 1.58257, Train Acc: 0.40370, Test Acc: 0.33333\n",
            "Epoch: 097, Loss: 1.58164, Train Acc: 0.40370, Test Acc: 0.35000\n",
            "Epoch: 098, Loss: 1.58168, Train Acc: 0.40556, Test Acc: 0.36667\n",
            "Epoch: 099, Loss: 1.58151, Train Acc: 0.40185, Test Acc: 0.35000\n",
            "Epoch: 100, Loss: 1.58179, Train Acc: 0.41111, Test Acc: 0.35000\n",
            "Epoch: 101, Loss: 1.58204, Train Acc: 0.40741, Test Acc: 0.38333\n",
            "Epoch: 102, Loss: 1.58171, Train Acc: 0.39074, Test Acc: 0.35000\n",
            "Epoch: 103, Loss: 1.58195, Train Acc: 0.38333, Test Acc: 0.35000\n",
            "Epoch: 104, Loss: 1.58194, Train Acc: 0.37593, Test Acc: 0.33333\n",
            "Epoch: 105, Loss: 1.58245, Train Acc: 0.37778, Test Acc: 0.33333\n",
            "Epoch: 106, Loss: 1.58135, Train Acc: 0.41296, Test Acc: 0.31667\n",
            "Epoch: 107, Loss: 1.58126, Train Acc: 0.42222, Test Acc: 0.35000\n",
            "Epoch: 108, Loss: 1.58089, Train Acc: 0.42222, Test Acc: 0.33333\n",
            "Epoch: 109, Loss: 1.58026, Train Acc: 0.42407, Test Acc: 0.35000\n",
            "Epoch: 110, Loss: 1.57961, Train Acc: 0.42037, Test Acc: 0.41667\n",
            "Epoch: 111, Loss: 1.57939, Train Acc: 0.43148, Test Acc: 0.41667\n",
            "Epoch: 112, Loss: 1.57827, Train Acc: 0.42593, Test Acc: 0.41667\n",
            "Epoch: 113, Loss: 1.57569, Train Acc: 0.41852, Test Acc: 0.40000\n",
            "Epoch: 114, Loss: 1.57321, Train Acc: 0.41852, Test Acc: 0.40000\n",
            "Epoch: 115, Loss: 1.57150, Train Acc: 0.41481, Test Acc: 0.36667\n",
            "Epoch: 116, Loss: 1.57019, Train Acc: 0.40926, Test Acc: 0.36667\n",
            "Epoch: 117, Loss: 1.56942, Train Acc: 0.41667, Test Acc: 0.36667\n",
            "Epoch: 118, Loss: 1.56901, Train Acc: 0.40741, Test Acc: 0.36667\n",
            "Epoch: 119, Loss: 1.56893, Train Acc: 0.41481, Test Acc: 0.36667\n",
            "Epoch: 120, Loss: 1.56847, Train Acc: 0.40556, Test Acc: 0.35000\n",
            "Epoch: 121, Loss: 1.56883, Train Acc: 0.40185, Test Acc: 0.35000\n",
            "Epoch: 122, Loss: 1.56817, Train Acc: 0.40185, Test Acc: 0.36667\n",
            "Epoch: 123, Loss: 1.56854, Train Acc: 0.40185, Test Acc: 0.31667\n",
            "Epoch: 124, Loss: 1.56962, Train Acc: 0.39259, Test Acc: 0.30000\n",
            "Epoch: 125, Loss: 1.57014, Train Acc: 0.38889, Test Acc: 0.31667\n",
            "Epoch: 126, Loss: 1.57003, Train Acc: 0.39630, Test Acc: 0.30000\n",
            "Epoch: 127, Loss: 1.56973, Train Acc: 0.40370, Test Acc: 0.28333\n",
            "Epoch: 128, Loss: 1.57022, Train Acc: 0.40926, Test Acc: 0.28333\n",
            "Epoch: 129, Loss: 1.57181, Train Acc: 0.39815, Test Acc: 0.33333\n",
            "Epoch: 130, Loss: 1.57452, Train Acc: 0.40556, Test Acc: 0.31667\n",
            "Epoch: 131, Loss: 1.57201, Train Acc: 0.40000, Test Acc: 0.31667\n",
            "Epoch: 132, Loss: 1.56968, Train Acc: 0.39815, Test Acc: 0.35000\n",
            "Epoch: 133, Loss: 1.56920, Train Acc: 0.39259, Test Acc: 0.35000\n",
            "Epoch: 134, Loss: 1.56878, Train Acc: 0.41296, Test Acc: 0.35000\n",
            "Epoch: 135, Loss: 1.56576, Train Acc: 0.40556, Test Acc: 0.36667\n",
            "Epoch: 136, Loss: 1.56367, Train Acc: 0.41481, Test Acc: 0.33333\n",
            "Epoch: 137, Loss: 1.56243, Train Acc: 0.41111, Test Acc: 0.31667\n",
            "Epoch: 138, Loss: 1.56102, Train Acc: 0.41667, Test Acc: 0.33333\n",
            "Epoch: 139, Loss: 1.55964, Train Acc: 0.41667, Test Acc: 0.33333\n",
            "Epoch: 140, Loss: 1.55881, Train Acc: 0.41111, Test Acc: 0.33333\n",
            "Epoch: 141, Loss: 1.55833, Train Acc: 0.40556, Test Acc: 0.35000\n",
            "Epoch: 142, Loss: 1.55775, Train Acc: 0.40926, Test Acc: 0.30000\n",
            "Epoch: 143, Loss: 1.55740, Train Acc: 0.41111, Test Acc: 0.31667\n",
            "Epoch: 144, Loss: 1.55774, Train Acc: 0.38889, Test Acc: 0.31667\n",
            "Epoch: 145, Loss: 1.55991, Train Acc: 0.38889, Test Acc: 0.31667\n",
            "Epoch: 146, Loss: 1.56354, Train Acc: 0.40000, Test Acc: 0.26667\n",
            "Epoch: 147, Loss: 1.56662, Train Acc: 0.40370, Test Acc: 0.25000\n",
            "Epoch: 148, Loss: 1.56878, Train Acc: 0.40926, Test Acc: 0.30000\n",
            "Epoch: 149, Loss: 1.56730, Train Acc: 0.42407, Test Acc: 0.36667\n",
            "Epoch: 150, Loss: 1.56220, Train Acc: 0.40741, Test Acc: 0.38333\n",
            "Epoch: 151, Loss: 1.55973, Train Acc: 0.40370, Test Acc: 0.38333\n",
            "Epoch: 152, Loss: 1.55761, Train Acc: 0.41111, Test Acc: 0.38333\n",
            "Epoch: 153, Loss: 1.55530, Train Acc: 0.40185, Test Acc: 0.38333\n",
            "Epoch: 154, Loss: 1.55271, Train Acc: 0.39815, Test Acc: 0.36667\n",
            "Epoch: 155, Loss: 1.55177, Train Acc: 0.40370, Test Acc: 0.36667\n",
            "Epoch: 156, Loss: 1.55154, Train Acc: 0.40185, Test Acc: 0.33333\n",
            "Epoch: 157, Loss: 1.55398, Train Acc: 0.39630, Test Acc: 0.31667\n",
            "Epoch: 158, Loss: 1.55784, Train Acc: 0.40185, Test Acc: 0.31667\n",
            "Epoch: 159, Loss: 1.55951, Train Acc: 0.41296, Test Acc: 0.35000\n",
            "Epoch: 160, Loss: 1.55625, Train Acc: 0.41852, Test Acc: 0.36667\n",
            "Epoch: 161, Loss: 1.55240, Train Acc: 0.39815, Test Acc: 0.40000\n",
            "Epoch: 162, Loss: 1.55116, Train Acc: 0.39630, Test Acc: 0.38333\n",
            "Epoch: 163, Loss: 1.54972, Train Acc: 0.39630, Test Acc: 0.35000\n",
            "Epoch: 164, Loss: 1.54976, Train Acc: 0.40185, Test Acc: 0.35000\n",
            "Epoch: 165, Loss: 1.54994, Train Acc: 0.40000, Test Acc: 0.33333\n",
            "Epoch: 166, Loss: 1.55100, Train Acc: 0.40185, Test Acc: 0.36667\n",
            "Epoch: 167, Loss: 1.55181, Train Acc: 0.39815, Test Acc: 0.36667\n",
            "Epoch: 168, Loss: 1.55278, Train Acc: 0.39444, Test Acc: 0.38333\n",
            "Epoch: 169, Loss: 1.55327, Train Acc: 0.40000, Test Acc: 0.38333\n",
            "Epoch: 170, Loss: 1.55312, Train Acc: 0.40185, Test Acc: 0.31667\n",
            "Epoch: 171, Loss: 1.55212, Train Acc: 0.40185, Test Acc: 0.35000\n",
            "Epoch: 172, Loss: 1.55074, Train Acc: 0.39815, Test Acc: 0.33333\n",
            "Epoch: 173, Loss: 1.54958, Train Acc: 0.39259, Test Acc: 0.35000\n",
            "Epoch: 174, Loss: 1.54989, Train Acc: 0.39630, Test Acc: 0.33333\n",
            "Epoch: 175, Loss: 1.55072, Train Acc: 0.39259, Test Acc: 0.33333\n",
            "Epoch: 176, Loss: 1.55232, Train Acc: 0.39074, Test Acc: 0.33333\n",
            "Epoch: 177, Loss: 1.55409, Train Acc: 0.38704, Test Acc: 0.33333\n",
            "Epoch: 178, Loss: 1.55991, Train Acc: 0.39259, Test Acc: 0.28333\n",
            "Epoch: 179, Loss: 1.56284, Train Acc: 0.40370, Test Acc: 0.31667\n",
            "Epoch: 180, Loss: 1.55716, Train Acc: 0.40926, Test Acc: 0.33333\n",
            "Epoch: 181, Loss: 1.54981, Train Acc: 0.40370, Test Acc: 0.33333\n",
            "Epoch: 182, Loss: 1.54645, Train Acc: 0.41111, Test Acc: 0.35000\n",
            "Epoch: 183, Loss: 1.54389, Train Acc: 0.41852, Test Acc: 0.33333\n",
            "Epoch: 184, Loss: 1.54369, Train Acc: 0.40185, Test Acc: 0.33333\n",
            "Epoch: 185, Loss: 1.54230, Train Acc: 0.39444, Test Acc: 0.33333\n",
            "Epoch: 186, Loss: 1.54246, Train Acc: 0.39074, Test Acc: 0.35000\n",
            "Epoch: 187, Loss: 1.54255, Train Acc: 0.39074, Test Acc: 0.35000\n",
            "Epoch: 188, Loss: 1.54238, Train Acc: 0.38889, Test Acc: 0.35000\n",
            "Epoch: 189, Loss: 1.54205, Train Acc: 0.40926, Test Acc: 0.35000\n",
            "Epoch: 190, Loss: 1.54096, Train Acc: 0.40370, Test Acc: 0.33333\n",
            "Epoch: 191, Loss: 1.53962, Train Acc: 0.41296, Test Acc: 0.35000\n",
            "Epoch: 192, Loss: 1.53802, Train Acc: 0.40926, Test Acc: 0.35000\n",
            "Epoch: 193, Loss: 1.53730, Train Acc: 0.41296, Test Acc: 0.35000\n",
            "Epoch: 194, Loss: 1.53603, Train Acc: 0.41481, Test Acc: 0.35000\n",
            "Epoch: 195, Loss: 1.53529, Train Acc: 0.41296, Test Acc: 0.35000\n",
            "Epoch: 196, Loss: 1.53464, Train Acc: 0.42407, Test Acc: 0.33333\n",
            "Epoch: 197, Loss: 1.53395, Train Acc: 0.41296, Test Acc: 0.33333\n",
            "Epoch: 198, Loss: 1.53371, Train Acc: 0.42037, Test Acc: 0.31667\n",
            "Epoch: 199, Loss: 1.53356, Train Acc: 0.40741, Test Acc: 0.35000\n",
            "Epoch: 200, Loss: 1.53292, Train Acc: 0.41667, Test Acc: 0.33333\n",
            "Epoch: 201, Loss: 1.53316, Train Acc: 0.41296, Test Acc: 0.33333\n",
            "Epoch: 202, Loss: 1.53235, Train Acc: 0.41481, Test Acc: 0.33333\n",
            "Epoch: 203, Loss: 1.53264, Train Acc: 0.41111, Test Acc: 0.36667\n",
            "Epoch: 204, Loss: 1.53203, Train Acc: 0.40556, Test Acc: 0.33333\n",
            "Epoch: 205, Loss: 1.53243, Train Acc: 0.40556, Test Acc: 0.36667\n",
            "Epoch: 206, Loss: 1.53228, Train Acc: 0.41111, Test Acc: 0.35000\n",
            "Epoch: 207, Loss: 1.53319, Train Acc: 0.42037, Test Acc: 0.31667\n",
            "Epoch: 208, Loss: 1.53334, Train Acc: 0.40926, Test Acc: 0.35000\n",
            "Epoch: 209, Loss: 1.53368, Train Acc: 0.41481, Test Acc: 0.36667\n",
            "Epoch: 210, Loss: 1.53371, Train Acc: 0.42037, Test Acc: 0.36667\n",
            "Epoch: 211, Loss: 1.53338, Train Acc: 0.42037, Test Acc: 0.36667\n",
            "Epoch: 212, Loss: 1.53250, Train Acc: 0.42037, Test Acc: 0.35000\n",
            "Epoch: 213, Loss: 1.53156, Train Acc: 0.42037, Test Acc: 0.36667\n",
            "Epoch: 214, Loss: 1.53039, Train Acc: 0.41667, Test Acc: 0.41667\n",
            "Epoch: 215, Loss: 1.52895, Train Acc: 0.41667, Test Acc: 0.36667\n",
            "Epoch: 216, Loss: 1.52923, Train Acc: 0.41852, Test Acc: 0.38333\n",
            "Epoch: 217, Loss: 1.52813, Train Acc: 0.42963, Test Acc: 0.41667\n",
            "Epoch: 218, Loss: 1.52888, Train Acc: 0.42222, Test Acc: 0.40000\n",
            "Epoch: 219, Loss: 1.52878, Train Acc: 0.42407, Test Acc: 0.38333\n",
            "Epoch: 220, Loss: 1.52922, Train Acc: 0.43704, Test Acc: 0.36667\n",
            "Epoch: 221, Loss: 1.53000, Train Acc: 0.42037, Test Acc: 0.35000\n",
            "Epoch: 222, Loss: 1.53195, Train Acc: 0.42222, Test Acc: 0.36667\n",
            "Epoch: 223, Loss: 1.53357, Train Acc: 0.41296, Test Acc: 0.35000\n",
            "Epoch: 224, Loss: 1.53945, Train Acc: 0.37963, Test Acc: 0.28333\n",
            "Epoch: 225, Loss: 1.55077, Train Acc: 0.38889, Test Acc: 0.31667\n",
            "Epoch: 226, Loss: 1.55078, Train Acc: 0.42222, Test Acc: 0.33333\n",
            "Epoch: 227, Loss: 1.54045, Train Acc: 0.41667, Test Acc: 0.30000\n",
            "Epoch: 228, Loss: 1.54435, Train Acc: 0.42222, Test Acc: 0.26667\n",
            "Epoch: 229, Loss: 1.54509, Train Acc: 0.42037, Test Acc: 0.30000\n",
            "Epoch: 230, Loss: 1.53590, Train Acc: 0.40185, Test Acc: 0.31667\n",
            "Epoch: 231, Loss: 1.52779, Train Acc: 0.41852, Test Acc: 0.33333\n",
            "Epoch: 232, Loss: 1.52608, Train Acc: 0.42407, Test Acc: 0.33333\n",
            "Epoch: 233, Loss: 1.52403, Train Acc: 0.41667, Test Acc: 0.36667\n",
            "Epoch: 234, Loss: 1.52420, Train Acc: 0.41852, Test Acc: 0.36667\n",
            "Epoch: 235, Loss: 1.52482, Train Acc: 0.41667, Test Acc: 0.36667\n",
            "Epoch: 236, Loss: 1.52610, Train Acc: 0.40926, Test Acc: 0.35000\n",
            "Epoch: 237, Loss: 1.52776, Train Acc: 0.40185, Test Acc: 0.36667\n",
            "Epoch: 238, Loss: 1.53119, Train Acc: 0.41481, Test Acc: 0.33333\n",
            "Epoch: 239, Loss: 1.53201, Train Acc: 0.40370, Test Acc: 0.31667\n",
            "Epoch: 240, Loss: 1.53209, Train Acc: 0.40185, Test Acc: 0.33333\n",
            "Epoch: 241, Loss: 1.53170, Train Acc: 0.40741, Test Acc: 0.31667\n",
            "Epoch: 242, Loss: 1.52921, Train Acc: 0.40556, Test Acc: 0.33333\n",
            "Epoch: 243, Loss: 1.52820, Train Acc: 0.41296, Test Acc: 0.33333\n",
            "Epoch: 244, Loss: 1.52922, Train Acc: 0.42407, Test Acc: 0.31667\n",
            "Epoch: 245, Loss: 1.53127, Train Acc: 0.43333, Test Acc: 0.28333\n",
            "Epoch: 246, Loss: 1.53130, Train Acc: 0.42963, Test Acc: 0.33333\n",
            "Epoch: 247, Loss: 1.53095, Train Acc: 0.43333, Test Acc: 0.33333\n",
            "Epoch: 248, Loss: 1.52501, Train Acc: 0.41667, Test Acc: 0.30000\n",
            "Epoch: 249, Loss: 1.52472, Train Acc: 0.42963, Test Acc: 0.33333\n",
            "Epoch: 250, Loss: 1.52287, Train Acc: 0.41852, Test Acc: 0.35000\n",
            "Epoch: 251, Loss: 1.52177, Train Acc: 0.41667, Test Acc: 0.31667\n",
            "Epoch: 252, Loss: 1.52090, Train Acc: 0.42222, Test Acc: 0.35000\n",
            "Epoch: 253, Loss: 1.51928, Train Acc: 0.41852, Test Acc: 0.35000\n",
            "Epoch: 254, Loss: 1.51888, Train Acc: 0.42963, Test Acc: 0.33333\n",
            "Epoch: 255, Loss: 1.51685, Train Acc: 0.42593, Test Acc: 0.35000\n",
            "Epoch: 256, Loss: 1.51688, Train Acc: 0.43704, Test Acc: 0.35000\n",
            "Epoch: 257, Loss: 1.51552, Train Acc: 0.43519, Test Acc: 0.36667\n",
            "Epoch: 258, Loss: 1.51652, Train Acc: 0.44259, Test Acc: 0.33333\n",
            "Epoch: 259, Loss: 1.51502, Train Acc: 0.43333, Test Acc: 0.33333\n",
            "Epoch: 260, Loss: 1.51648, Train Acc: 0.43148, Test Acc: 0.31667\n",
            "Epoch: 261, Loss: 1.51465, Train Acc: 0.43333, Test Acc: 0.36667\n",
            "Epoch: 262, Loss: 1.51646, Train Acc: 0.44815, Test Acc: 0.31667\n",
            "Epoch: 263, Loss: 1.51415, Train Acc: 0.41852, Test Acc: 0.36667\n",
            "Epoch: 264, Loss: 1.51421, Train Acc: 0.42963, Test Acc: 0.38333\n",
            "Epoch: 265, Loss: 1.51307, Train Acc: 0.42963, Test Acc: 0.35000\n",
            "Epoch: 266, Loss: 1.51268, Train Acc: 0.42778, Test Acc: 0.31667\n",
            "Epoch: 267, Loss: 1.51305, Train Acc: 0.43148, Test Acc: 0.30000\n",
            "Epoch: 268, Loss: 1.51201, Train Acc: 0.42778, Test Acc: 0.35000\n",
            "Epoch: 269, Loss: 1.51383, Train Acc: 0.42963, Test Acc: 0.33333\n",
            "Epoch: 270, Loss: 1.51474, Train Acc: 0.42222, Test Acc: 0.36667\n",
            "Epoch: 271, Loss: 1.51523, Train Acc: 0.43519, Test Acc: 0.33333\n",
            "Epoch: 272, Loss: 1.51575, Train Acc: 0.42778, Test Acc: 0.36667\n",
            "Epoch: 273, Loss: 1.51526, Train Acc: 0.41481, Test Acc: 0.33333\n",
            "Epoch: 274, Loss: 1.51737, Train Acc: 0.43333, Test Acc: 0.38333\n",
            "Epoch: 275, Loss: 1.51690, Train Acc: 0.40000, Test Acc: 0.33333\n",
            "Epoch: 276, Loss: 1.51428, Train Acc: 0.42037, Test Acc: 0.40000\n",
            "Epoch: 277, Loss: 1.51490, Train Acc: 0.41481, Test Acc: 0.35000\n",
            "Epoch: 278, Loss: 1.51285, Train Acc: 0.42037, Test Acc: 0.36667\n",
            "Epoch: 279, Loss: 1.51394, Train Acc: 0.40556, Test Acc: 0.38333\n",
            "Epoch: 280, Loss: 1.51565, Train Acc: 0.40370, Test Acc: 0.35000\n",
            "Epoch: 281, Loss: 1.51757, Train Acc: 0.39815, Test Acc: 0.31667\n",
            "Epoch: 282, Loss: 1.52173, Train Acc: 0.38889, Test Acc: 0.28333\n",
            "Epoch: 283, Loss: 1.52507, Train Acc: 0.38333, Test Acc: 0.31667\n",
            "Epoch: 284, Loss: 1.53136, Train Acc: 0.38519, Test Acc: 0.35000\n",
            "Epoch: 285, Loss: 1.53706, Train Acc: 0.33704, Test Acc: 0.31667\n",
            "Epoch: 286, Loss: 1.54177, Train Acc: 0.34259, Test Acc: 0.30000\n",
            "Epoch: 287, Loss: 1.53479, Train Acc: 0.40556, Test Acc: 0.35000\n",
            "Epoch: 288, Loss: 1.52430, Train Acc: 0.40741, Test Acc: 0.36667\n",
            "Epoch: 289, Loss: 1.52284, Train Acc: 0.41852, Test Acc: 0.35000\n",
            "Epoch: 290, Loss: 1.52191, Train Acc: 0.39630, Test Acc: 0.31667\n",
            "Epoch: 291, Loss: 1.52390, Train Acc: 0.38333, Test Acc: 0.31667\n",
            "Epoch: 292, Loss: 1.52550, Train Acc: 0.37963, Test Acc: 0.30000\n",
            "Epoch: 293, Loss: 1.52321, Train Acc: 0.38889, Test Acc: 0.28333\n",
            "Epoch: 294, Loss: 1.51804, Train Acc: 0.39815, Test Acc: 0.30000\n",
            "Epoch: 295, Loss: 1.51352, Train Acc: 0.39815, Test Acc: 0.28333\n",
            "Epoch: 296, Loss: 1.50990, Train Acc: 0.39259, Test Acc: 0.28333\n",
            "Epoch: 297, Loss: 1.50914, Train Acc: 0.38889, Test Acc: 0.28333\n",
            "Epoch: 298, Loss: 1.50878, Train Acc: 0.39444, Test Acc: 0.30000\n",
            "Epoch: 299, Loss: 1.50948, Train Acc: 0.39815, Test Acc: 0.30000\n",
            "Epoch: 300, Loss: 1.51223, Train Acc: 0.37407, Test Acc: 0.31667\n",
            "Epoch: 301, Loss: 1.51345, Train Acc: 0.37037, Test Acc: 0.30000\n",
            "Epoch: 302, Loss: 1.52005, Train Acc: 0.36296, Test Acc: 0.31667\n",
            "Epoch: 303, Loss: 1.52556, Train Acc: 0.37593, Test Acc: 0.35000\n",
            "Epoch: 304, Loss: 1.52438, Train Acc: 0.39444, Test Acc: 0.36667\n",
            "Epoch: 305, Loss: 1.51715, Train Acc: 0.40741, Test Acc: 0.35000\n",
            "Epoch: 306, Loss: 1.50966, Train Acc: 0.39630, Test Acc: 0.35000\n",
            "Epoch: 307, Loss: 1.50678, Train Acc: 0.38704, Test Acc: 0.35000\n",
            "Epoch: 308, Loss: 1.50686, Train Acc: 0.38519, Test Acc: 0.33333\n",
            "Epoch: 309, Loss: 1.50794, Train Acc: 0.38333, Test Acc: 0.30000\n",
            "Epoch: 310, Loss: 1.50814, Train Acc: 0.37407, Test Acc: 0.31667\n",
            "Epoch: 311, Loss: 1.50965, Train Acc: 0.39444, Test Acc: 0.31667\n",
            "Epoch: 312, Loss: 1.50991, Train Acc: 0.37593, Test Acc: 0.30000\n",
            "Epoch: 313, Loss: 1.51141, Train Acc: 0.39074, Test Acc: 0.33333\n",
            "Epoch: 314, Loss: 1.51227, Train Acc: 0.37778, Test Acc: 0.30000\n",
            "Epoch: 315, Loss: 1.51167, Train Acc: 0.39444, Test Acc: 0.35000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2a2cd9fccd0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2a2cd9fccd0b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8f679d176356>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# x = F.dropout(x, training=self.training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# x = F.dropout(x, training=self.training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CliquePoolingLayer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, batch, attn)\u001b[0m\n\u001b[1;32m    277\u001b[0m           \u001b[0mconsidered_cliques\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m           \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m           \u001b[0mnodes_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m       )\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CliquePoolingLayer.py\u001b[0m in \u001b[0;36mget_dual_edges\u001b[0;34m(edge_index, considered_cliques, clusters, nodes_num)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     neighbourhood = { \n\u001b[0;32m--> 187\u001b[0;31m       \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCliquePooling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_neighbours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     }\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CliquePoolingLayer.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     neighbourhood = { \n\u001b[0;32m--> 187\u001b[0;31m       \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCliquePooling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_neighbours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     }\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CliquePoolingLayer.py\u001b[0m in \u001b[0;36mget_neighbours\u001b[0;34m(edge_index, v)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mneighboring\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0mof\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCliquePooling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     return set(\n",
            "\u001b[0;32m/content/CliquePoolingLayer.py\u001b[0m in \u001b[0;36mget_edges\u001b[0;34m(edge_index)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CRdTG6X1IfW"
      },
      "source": [
        "### Ablation study w.r.t MUTAG dataset\n",
        "---\n",
        "Firstly I apply just 3 convolutional layers without pooling and then after 2nd and 3rd convolution layer, I apply one CliquePooling layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C68Jbby1rGv",
        "outputId": "a35e9615-64f4-4f25-8f07-1908e44baa04"
      },
      "source": [
        "# https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing#scrollTo=CN3sRVuaQ88l\n",
        "\n",
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting data/TUDataset/MUTAG/MUTAG.zip\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "Dataset: MUTAG(188):\n",
            "====================\n",
            "Number of graphs: 188\n",
            "Number of features: 7\n",
            "Number of classes: 2\n",
            "\n",
            "Data(edge_attr=[38, 4], edge_index=[2, 38], x=[17, 7], y=[1])\n",
            "=============================================================\n",
            "Number of nodes: 17\n",
            "Number of edges: 38\n",
            "Average node degree: 2.24\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV0Y6jcf1rGw",
        "outputId": "8dcd6bfb-8001-48ef-c470-ad9261a19b76"
      },
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "b_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=b_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=b_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZlaxLxU1rGx",
        "outputId": "a9410bde-bac9-4da9-d78f-ad79c852c098"
      },
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super(GCN, self).__init__()\n",
        "    torch.manual_seed(12345)\n",
        "    self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "    self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "    self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "    self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    # 1. Obtain node embeddings \n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.conv3(x, edge_index)\n",
        "\n",
        "    # 2. Readout layer\n",
        "    x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "    # 3. Apply a final classifier\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.lin(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "A36yMIJD4Adp",
        "outputId": "70eb9048-8b61-4091-e480-d2ca15275d28"
      },
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "  model.train()\n",
        "\n",
        "  for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "    out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "    loss = criterion(out, data.y)  # Compute the loss.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "    out = model(data.x, data.edge_index, data.batch)  \n",
        "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "    correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "  return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "  train()\n",
        "  train_acc = test(train_loader)\n",
        "  test_acc = test(test_loader)\n",
        "  print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 002, Train Acc: 0.6800, Test Acc: 0.8158\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 012, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 013, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 014, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 015, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 016, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 017, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 018, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 019, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 020, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 021, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 022, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 023, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 024, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 025, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 026, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 027, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 028, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 029, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 030, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 031, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 032, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 033, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 034, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 035, Train Acc: 0.6600, Test Acc: 0.7632\n",
            "Epoch: 036, Train Acc: 0.6800, Test Acc: 0.7632\n",
            "Epoch: 037, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 038, Train Acc: 0.6933, Test Acc: 0.7895\n",
            "Epoch: 039, Train Acc: 0.6800, Test Acc: 0.7895\n",
            "Epoch: 040, Train Acc: 0.6800, Test Acc: 0.7632\n",
            "Epoch: 041, Train Acc: 0.6800, Test Acc: 0.7895\n",
            "Epoch: 042, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 043, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 044, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 045, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 046, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 047, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 048, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 049, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 050, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 051, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 052, Train Acc: 0.7067, Test Acc: 0.7632\n",
            "Epoch: 053, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 054, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 055, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 056, Train Acc: 0.6933, Test Acc: 0.7895\n",
            "Epoch: 057, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 058, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 059, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 060, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 061, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 062, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 063, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 064, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 065, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 066, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 067, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 068, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 069, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 070, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 071, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 072, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 073, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 074, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 075, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 076, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 077, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 078, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 079, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 080, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 081, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 082, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 083, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 084, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 085, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 086, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 087, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 088, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 089, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 090, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 091, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 092, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 093, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 094, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 095, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 096, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 097, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 098, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 099, Train Acc: 0.7600, Test Acc: 0.7895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-imOd1y2U3a"
      },
      "source": [
        "#### + adding pooling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Du4krpc8z5",
        "outputId": "d228ed90-6ef5-417e-c69c-36416177e190"
      },
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super(GCN, self).__init__()\n",
        "    torch.manual_seed(12345)\n",
        "    self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "    self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "    self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "    self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "    self.pool  = CliquePooling('avg')\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    # 1. Obtain node embeddings \n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = self.conv2(x, edge_index)\n",
        "    d_x, d_edge_index, _, dual_batch, _ = self.pool(x, edge_index, None, batch)\n",
        "    x = d_x.relu()\n",
        "    x = self.conv3(x, edge_index)\n",
        "    d_x, d_edge_index, _, dual_batch, _ = self.pool(x, edge_index, None, batch)\n",
        "    # \n",
        "\n",
        "    # 2. Readout layer\n",
        "    x = global_mean_pool(d_x, dual_batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "    # 3. Apply a final classifier\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.lin(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (pool): CliquePooling(agg_type=avg)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "AbQA_f_Ac8z9",
        "outputId": "2357f09e-d838-4fc8-bd25-ce6083629dda"
      },
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, eps=0.0001, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "  model.train()\n",
        "\n",
        "  for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "    out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "    loss = criterion(out, data.y)  # Compute the loss.\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "  model.eval()\n",
        "\n",
        "  correct = 0\n",
        "  for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "    out = model(data.x, data.edge_index, data.batch)  \n",
        "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "    correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "  return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "  train()\n",
        "  train_acc = test(train_loader)\n",
        "  test_acc = test(test_loader)\n",
        "  print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001, Train Acc: 0.3533, Test Acc: 0.2632\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7632\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 008, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 012, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 013, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 014, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 015, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 016, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 017, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 018, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 019, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 020, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 021, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 022, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 023, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 024, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 025, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 026, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 027, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 028, Train Acc: 0.6533, Test Acc: 0.7632\n",
            "Epoch: 029, Train Acc: 0.6600, Test Acc: 0.7632\n",
            "Epoch: 030, Train Acc: 0.6933, Test Acc: 0.7895\n",
            "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 032, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 033, Train Acc: 0.7333, Test Acc: 0.7632\n",
            "Epoch: 034, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 035, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 036, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 037, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 038, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 039, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 040, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 041, Train Acc: 0.7067, Test Acc: 0.7895\n",
            "Epoch: 042, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 043, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 044, Train Acc: 0.7133, Test Acc: 0.7632\n",
            "Epoch: 045, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 046, Train Acc: 0.7000, Test Acc: 0.7895\n",
            "Epoch: 047, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 048, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 049, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 050, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 051, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 052, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 053, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 054, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 055, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 056, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 057, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 058, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 059, Train Acc: 0.7267, Test Acc: 0.7895\n",
            "Epoch: 060, Train Acc: 0.7333, Test Acc: 0.7895\n",
            "Epoch: 061, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 062, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 063, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 064, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 065, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 066, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 067, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 068, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 069, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 070, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 071, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 072, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 073, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 074, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 075, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 076, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 077, Train Acc: 0.7533, Test Acc: 0.8158\n",
            "Epoch: 078, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 079, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 080, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 081, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 082, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 083, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 084, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 085, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 086, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 087, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 088, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 089, Train Acc: 0.7800, Test Acc: 0.8421\n",
            "Epoch: 090, Train Acc: 0.7667, Test Acc: 0.8421\n",
            "Epoch: 091, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 092, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 093, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 094, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 095, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 096, Train Acc: 0.7867, Test Acc: 0.8158\n",
            "Epoch: 097, Train Acc: 0.7867, Test Acc: 0.8421\n",
            "Epoch: 098, Train Acc: 0.7867, Test Acc: 0.8421\n",
            "Epoch: 099, Train Acc: 0.7867, Test Acc: 0.8421\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}